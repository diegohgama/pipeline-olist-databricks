{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e67c28c-08c9-43b8-b5f5-09dfed00cb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# 1. Configura√ß√£o dos caminhos\n",
    "# Baseado na sua imagem, este √© o local dos seus CSVs\n",
    "caminho_origem = \"/Volumes/workspace/default/arquivos_pipe/\"\n",
    "\n",
    "# Vamos listar os arquivos para garantir que o Databricks est√° vendo tudo\n",
    "arquivos = dbutils.fs.ls(caminho_origem)\n",
    "\n",
    "display(arquivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586dd286-afdc-451c-a53e-6a258325070a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# 1. Configura√ß√£o dos caminhos\n",
    "caminho_origem = \"/Volumes/workspace/default/arquivos_pipe/\"\n",
    "\n",
    "# 2. Fun√ß√£o de Ingest√£o Gen√©rica (Corrigida para Unity Catalog)\n",
    "def ingerir_tabela(nome_arquivo, nome_tabela):\n",
    "    print(f\"Iniciando ingest√£o de: {nome_tabela}...\")\n",
    "    \n",
    "    # Lendo o CSV\n",
    "    df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .option(\"delimiter\", \",\")\n",
    "          .load(f\"{caminho_origem}{nome_arquivo}\")\n",
    "    )\n",
    "    \n",
    "    # CORRE√á√ÉO AQUI: Usando _metadata.file_path ao inv√©s de input_file_name()\n",
    "    # O asterisco \"*\" garante que todas as colunas originais sejam mantidas\n",
    "    df_bronze = df.select(\n",
    "        \"*\", \n",
    "        col(\"_metadata.file_path\").alias(\"arquivo_origem\")\n",
    "    ).withColumn(\"data_ingestao\", current_timestamp())\n",
    "    \n",
    "    # Salvando como tabela Delta\n",
    "    df_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"default.bronze_{nome_tabela}\")\n",
    "        \n",
    "    print(f\"Tabela bronze_{nome_tabela} salva com sucesso!\")\n",
    "\n",
    "# 3. Executando para todos os arquivos\n",
    "tabelas_para_criar = {\n",
    "    \"olist_customers_dataset.csv\": \"clientes\",\n",
    "    \"olist_geolocation_dataset.csv\": \"geolocalizacao\",\n",
    "    \"olist_order_items_dataset.csv\": \"itens_pedido\",\n",
    "    \"olist_order_payments_dataset.csv\": \"pagamentos\",\n",
    "    \"olist_order_reviews_dataset.csv\": \"avaliacoes\",\n",
    "    \"olist_orders_dataset.csv\": \"pedidos\",\n",
    "    \"olist_products_dataset.csv\": \"produtos\",\n",
    "    \"olist_sellers_dataset.csv\": \"vendedores\",\n",
    "    \"product_category_name_translation.csv\": \"traducao_categorias\"\n",
    "}\n",
    "\n",
    "# Loop de execu√ß√£o\n",
    "for arquivo_csv, nome_tabela in tabelas_para_criar.items():\n",
    "    ingerir_tabela(arquivo_csv, nome_tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d403ca-3d7e-4642-9d33-8f3399e35e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# --- 1. Criando a SILVER_PEDIDOS ---\n",
    "print(\"Criando tabela: SILVER_PEDIDOS...\")\n",
    "df_pedidos = spark.read.table(\"default.bronze_pedidos\")\n",
    "\n",
    "df_silver_pedidos = df_pedidos.select(\n",
    "    col(\"order_id\").alias(\"id_pedido\"),\n",
    "    col(\"customer_id\").alias(\"id_cliente\"),\n",
    "    col(\"order_status\").alias(\"status_pedido\"),\n",
    "    # Convertendo Strings para Timestamps\n",
    "    to_timestamp(col(\"order_purchase_timestamp\")).alias(\"data_compra\"),\n",
    "    to_timestamp(col(\"order_approved_at\")).alias(\"data_aprovacao\"),\n",
    "    to_timestamp(col(\"order_delivered_carrier_date\")).alias(\"data_envio_transportadora\"),\n",
    "    to_timestamp(col(\"order_delivered_customer_date\")).alias(\"data_entrega_cliente\"),\n",
    "    to_timestamp(col(\"order_estimated_delivery_date\")).alias(\"data_estimada_entrega\")\n",
    ")\n",
    "\n",
    "df_silver_pedidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.silver_pedidos\")\n",
    "print(\"‚úÖ Tabela silver_pedidos criada.\")\n",
    "\n",
    "# --- 2. Criando a SILVER_ITENS_PEDIDO ---\n",
    "print(\"Criando tabela: SILVER_ITENS_PEDIDO...\")\n",
    "df_itens = spark.read.table(\"default.bronze_itens_pedido\")\n",
    "\n",
    "df_silver_itens = df_itens.select(\n",
    "    col(\"order_id\").alias(\"id_pedido\"),\n",
    "    col(\"order_item_id\").alias(\"id_item_pedido\"),\n",
    "    col(\"product_id\").alias(\"id_produto\"),\n",
    "    col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "    # Convertendo pre√ßos para n√∫meros decimais\n",
    "    col(\"price\").cast(\"double\").alias(\"vl_preco\"),\n",
    "    col(\"freight_value\").cast(\"double\").alias(\"vl_frete\")\n",
    ")\n",
    "\n",
    "df_silver_itens.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.silver_itens_pedido\")\n",
    "print(\"‚úÖ Tabela silver_itens_pedido criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b70281-3180-4d56-9e88-91f1eb533522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, sum, round\n",
    "\n",
    "# 1. Leitura das tabelas Silver (Que agora existem!)\n",
    "df_pedidos = spark.read.table(\"default.silver_pedidos\")\n",
    "df_itens = spark.read.table(\"default.silver_itens_pedido\")\n",
    "\n",
    "# 2. O JOIN\n",
    "df_join = df_pedidos.join(df_itens, on=\"id_pedido\", how=\"inner\")\n",
    "\n",
    "# 3. Agrega√ß√£o (Vendas por Dia)\n",
    "df_gold = df_join.select(\n",
    "    to_date(col(\"data_compra\")).alias(\"data_venda\"),\n",
    "    col(\"vl_preco\"),\n",
    "    col(\"vl_frete\")\n",
    ").groupBy(\"data_venda\") \\\n",
    " .agg(\n",
    "     round(sum(\"vl_preco\"), 2).alias(\"total_vendas\"),\n",
    "     round(sum(\"vl_frete\"), 2).alias(\"total_frete\")\n",
    " ).orderBy(\"data_venda\")\n",
    "\n",
    "# 4. Salvando a Gold\n",
    "df_gold.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.gold_vendas_diarias\")\n",
    "\n",
    "display(df_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ce541c-7298-40de-886f-f4cfd5cd3b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Iniciando tratamento: SILVER_PRODUTOS...\")\n",
    "\n",
    "# 1. Leitura da Bronze\n",
    "df_produtos = spark.read.table(\"default.bronze_produtos\")\n",
    "\n",
    "# 2. Sele√ß√£o e Renomea√ß√£o (O cora√ß√£o da camada Silver)\n",
    "df_silver_produtos = df_produtos.select(\n",
    "    col(\"product_id\").alias(\"id_produto\"),\n",
    "    col(\"product_category_name\").alias(\"categoria\")\n",
    ")\n",
    "\n",
    "# 3. Escrita na Silver\n",
    "df_silver_produtos.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.silver_produtos\")\n",
    "\n",
    "print(\"‚úÖ Tabela silver_produtos criada com sucesso!\")\n",
    "display(df_silver_produtos.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca090af6-79d3-437e-82ac-86f24df474cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, sum, round, desc\n",
    "\n",
    "print(\"üìä Gerando Relat√≥rio: Campe√µes da Black Friday 2017...\")\n",
    "\n",
    "# 1. Leitura das 3 Tabelas Silver\n",
    "df_pedidos = spark.read.table(\"default.silver_pedidos\")\n",
    "df_itens = spark.read.table(\"default.silver_itens_pedido\")\n",
    "df_produtos = spark.read.table(\"default.silver_produtos\")\n",
    "\n",
    "# 2. O Join Triplo (Cascata)\n",
    "# Primeiro juntamos Pedidos com Itens...\n",
    "df_join_1 = df_pedidos.join(df_itens, on=\"id_pedido\", how=\"inner\")\n",
    "\n",
    "# ...e o resultado juntamos com Produtos\n",
    "df_completo = df_join_1.join(df_produtos, on=\"id_produto\", how=\"inner\")\n",
    "\n",
    "# 3. A Pergunta de Neg√≥cio: \"O que mais vendeu em 24/11/2017?\"\n",
    "df_relatorio = df_completo \\\n",
    "    .filter(to_date(col(\"data_compra\")) == \"2017-11-24\") \\\n",
    "    .groupBy(\"categoria\") \\\n",
    "    .agg(\n",
    "        round(sum(\"vl_preco\"), 2).alias(\"receita_total\"),\n",
    "        sum(\"vl_preco\").alias(\"check_ordenacao\") # Auxiliar\n",
    "    ) \\\n",
    "    .orderBy(col(\"check_ordenacao\").desc()) \\\n",
    "    .drop(\"check_ordenacao\") # Removemos a auxiliar para ficar limpo\n",
    "\n",
    "# 4. Exibindo o Top 10\n",
    "display(df_relatorio.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cluster-olist",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
